"functions":
- "name": "_Z26llama_init_from_gpt_paramsR10gpt_params"
  "params":
  - "name": "params"
    "type": "bool "
  - "name": ""
    "type": "bool "
  "return_type": "void"
  "signature": "struct llama_init_result llama_init_from_gpt_params(struct gpt_params &)"
- "name": "ggml_opt_resume_g"
  "params":
  - "name": "ctx"
    "type": "bool "
  - "name": "opt"
    "type": "bool "
  - "name": "f"
    "type": "bool "
  - "name": "gf"
    "type": "bool "
  - "name": "gb"
    "type": "bool "
  - "name": "callback"
    "type": "bool "
  - "name": "callback_data"
    "type": "bool "
  "return_type": "int"
  "signature": "DW_TAG_enumeration_typeggml_opt_result ggml_opt_resume_g(struct ggml_context *, struct ggml_opt_context *, struct ggml_tensor *, struct ggml_cgraph *, struct ggml_cgraph *, ggml_opt_callback, void *)"
- "name": "llama_model_quantize"
  "params":
  - "name": "fname_inp"
    "type": "bool "
  - "name": "fname_out"
    "type": "bool "
  - "name": "params"
    "type": "bool "
  "return_type": "int"
  "signature": "uint32_t llama_model_quantize(const char *, const char *, const llama_model_quantize_params *)"
- "name": "ggml_opt"
  "params":
  - "name": "ctx"
    "type": "bool "
  - "name": "params"
    "type": "bool "
  - "name": "f"
    "type": "bool "
  "return_type": "int"
  "signature": "DW_TAG_enumeration_typeggml_opt_result ggml_opt(struct ggml_context *, struct ggml_opt_params, struct ggml_tensor *)"
- "name": "ggml_opt_resume"
  "params":
  - "name": "ctx"
    "type": "bool "
  - "name": "opt"
    "type": "bool "
  - "name": "f"
    "type": "bool "
  "return_type": "int"
  "signature": "DW_TAG_enumeration_typeggml_opt_result ggml_opt_resume(struct ggml_context *, struct ggml_opt_context *, struct ggml_tensor *)"
"language": "c++"
"project": "llamacpp"
"target_name": "fuzz_apply_template"
"target_path": "/src/llama.cpp/fuzzers/fuzz_apply_template.cpp"
