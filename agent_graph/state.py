"""State management for LangGraph-based fuzzing workflow."""

from typing_extensions import TypedDict, NotRequired, Annotated
from typing import List, Dict, Any, Optional


def add_agent_messages(
    left: Dict[str, List[Dict[str, Any]]], 
    right: Dict[str, List[Dict[str, Any]]]
) -> Dict[str, List[Dict[str, Any]]]:
    """
    Message reducer for agent-specific messages.
    
    This reducer:
    1. Merges agent-specific message dictionaries
    2. Trims each agent's messages independently to 50k tokens
    3. Preserves system messages for each agent
    
    Args:
        left: Existing agent messages {agent_name: [messages]}
        right: New agent messages to merge
    
    Returns:
        Merged and trimmed agent messages
    """
    from agent_graph.memory import trim_messages_by_tokens
    
    # Start with a copy of left
    result = left.copy()
    
    # Merge each agent's messages from right
    for agent_name, messages in right.items():
        if agent_name in result:
            # Combine existing and new messages for this agent
            combined = result[agent_name] + messages
        else:
            # New agent, just use the messages from right
            combined = messages
        
        # Trim this agent's messages to 100k tokens
        result[agent_name] = trim_messages_by_tokens(
            combined,
            max_tokens=100000,  # Increase to 100k tokens per agent
            keep_system=True,
            system_max_tokens=10000  # Limit system message to 10k
        )
    
    return result

class FuzzingWorkflowState(TypedDict):
    """
    LangGraph state schema for the fuzzing workflow.
    
    This state schema is designed to be compatible with the original
    Result object hierarchy while providing structure for LangGraph.
    """
    
    # === Core Information (Required) ===
    benchmark: Dict[str, Any]  # Benchmark dict (serialized from experiment.benchmark.Benchmark)
    trial: int  # Trial number
    work_dirs: Dict[str, Any]  # WorkDirs dict (serialized from experiment.workdir.WorkDirs)
    
    # === Agent-Specific Messages ===
    # Each agent maintains its own conversation history independently
    # Format: {agent_name: [messages]}
    # Example: {"function_analyzer": [...], "prototyper": [...]}
    agent_messages: NotRequired[Annotated[Dict[str, List[Dict[str, Any]]], add_agent_messages]]
    
    # === Function Analysis (from FunctionAnalyzer) ===
    function_analysis: NotRequired[Dict[str, Any]]
    
    # === Context Analysis (from ContextAnalyzer) ===
    context_analysis: NotRequired[Dict[str, Any]]
    
    # === Build Results (from Prototyper/Enhancer) ===
    fuzz_target_source: NotRequired[str]
    build_script_source: NotRequired[str]
    compile_success: NotRequired[bool]
    build_errors: NotRequired[List[str]]  # Keep simple for compatibility
    compile_log: NotRequired[str]
    binary_exists: NotRequired[bool]
    is_function_referenced: NotRequired[bool]
    
    # === Execution Results (from ExecutionStage) ===
    run_success: NotRequired[bool]
    run_error: NotRequired[str]
    run_log: NotRequired[str]
    artifact_path: NotRequired[str]
    crash_func: NotRequired[str]
    crashes: NotRequired[bool]
    crash_info: NotRequired[Dict[str, Any]]  # Detailed crash information including artifact_path, stack_trace, etc.
    reproducer_path: NotRequired[str]
    
    # === Coverage Results ===
    coverage_summary: NotRequired[str]
    coverage_percent: NotRequired[float]
    line_coverage_diff: NotRequired[float]
    coverage_report_path: NotRequired[str]
    cov_pcs: NotRequired[int]
    total_pcs: NotRequired[int]
    no_coverage_improvement_count: NotRequired[int]  # Track consecutive iterations without coverage improvement
    
    # === Analysis Results (from Analyzers) ===
    analysis_complete: NotRequired[bool]
    crash_analysis: NotRequired[Dict[str, Any]]
    coverage_analysis: NotRequired[Dict[str, Any]]
    
    # === Workflow Control ===
    next_action: NotRequired[str]  # For supervisor routing
    retry_count: NotRequired[int]
    max_retries: NotRequired[int]
    supervisor_call_count: NotRequired[int]  # Global counter for supervisor invocations (loop prevention)
    node_visit_counts: NotRequired[Dict[str, int]]  # Per-node visit counter (loop prevention)
    workflow_phase: NotRequired[str]  # Current workflow phase: "compilation" or "optimization"
    compilation_retry_count: NotRequired[int]  # Separate counter for compilation retries
    prototyper_regenerate_count: NotRequired[int]  # Counter for prototyper regenerations
    previous_fuzz_target_source: NotRequired[str]  # Store previous version for diff generation
    
    # === Error Handling ===
    errors: NotRequired[List[Dict[str, Any]]]
    warnings: NotRequired[List[str]]
    
    # === Configuration (from agent_test.py compatibility) ===
    pipeline: NotRequired[List[str]]  # Agent pipeline
    use_context: NotRequired[bool]  # Whether to use context
    prompt_file: NotRequired[str]  # Path to prompt file
    additional_files_path: NotRequired[str]  # Path to additional files
    run_timeout: NotRequired[int]  # Execution timeout
    current_iteration: NotRequired[int]  # Current workflow iteration
    max_iterations: NotRequired[int]  # Maximum workflow iterations
    workflow_status: NotRequired[str]  # Workflow status
    active_containers: NotRequired[List[str]]  # Active container list
    crash_results: NotRequired[List[Dict[str, Any]]]  # Crash results list
    
    # === Token Usage Statistics ===
    token_usage: NotRequired[Dict[str, Any]]  # Token consumption statistics
    
    # === Session Memory (å…±è¯†çº¦æŸ) ===
    # æœ¬è½®ä»»åŠ¡å†…ï¼Œå„agentå·²è¾¾æˆçš„"å½“å‰å…±è¯†çº¦æŸ"
    # Supervisoråº”å§‹ç»ˆå¾€ä¸‹æ¸¸agentæ³¨å…¥è¿™ä¸ªå…±è¯†ï¼Œè€Œä¸æ˜¯æ•´ä¸ªæ¶ˆæ¯å†å²
    session_memory: NotRequired[Dict[str, Any]]

class WorkerState(TypedDict):
    """State for worker nodes in parallel execution."""
    
    task_id: str  # Unique task identifier
    task_type: str  # Type of task (compile, analyze, etc.)
    input_data: Dict[str, Any]  # Input data for the task
    output_data: NotRequired[Dict[str, Any]]  # Task output
    status: NotRequired[str]  # Task status
    error: NotRequired[str]  # Error message if task failed
    start_time: NotRequired[float]  # Task start timestamp
    end_time: NotRequired[float]  # Task completion timestamp

def create_initial_state(
    benchmark,  # experiment.benchmark.Benchmark object
    work_dirs,  # experiment.workdir.WorkDirs object
    trial: int = 0,
    max_round: int = 100,
    run_timeout: int = 300,
    pipeline: Optional[List[str]] = None,
    use_context: bool = False,
    prompt_file: str = "",
    additional_files_path: str = "",
    initial_prompt: str = "",
    model = None,
    **kwargs
) -> FuzzingWorkflowState:
    """Create an initial state for the fuzzing workflow with full parameter support."""
    
    # Serialize objects to dicts for msgpack compatibility
    benchmark_dict = benchmark.to_dict()
    work_dirs_dict = work_dirs.to_dict()
    
    # Initialize agent_messages dict (empty, agents will add their own system messages)
    agent_messages = {}
    
    return FuzzingWorkflowState(
        benchmark=benchmark_dict,
        trial=trial,
        work_dirs=work_dirs_dict,
        agent_messages=agent_messages,
        current_iteration=0,
        max_iterations=max_round,
        workflow_status="initialized",
        errors=[],
        warnings=[],
        # Loop prevention counters (similar to no_coverage_improvement_count)
        supervisor_call_count=0,
        node_visit_counts={},
        # Workflow phase control
        workflow_phase="compilation",  # Start with compilation phase
        compilation_retry_count=0,  # Track compilation retries separately
        prototyper_regenerate_count=0,  # Track prototyper regenerations
        previous_fuzz_target_source="",  # For diff generation
        # Store additional configuration
        pipeline=pipeline or [],
        use_context=use_context,
        prompt_file=prompt_file,
        additional_files_path=additional_files_path,
        run_timeout=run_timeout,
        build_errors=[],
        crash_results=[],
        active_containers=[],
        # Initialize token usage statistics
        token_usage={
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "total_tokens": 0,
            "by_agent": {}
        },
        # Initialize session memory (å…±è¯†çº¦æŸå­˜å‚¨)
        session_memory={
            "api_constraints": [],      # APIä½¿ç”¨çº¦æŸåˆ—è¡¨
            "archetype": None,           # å·²è¯†åˆ«çš„æ¶æ„æ¨¡å¼
            "known_fixes": [],           # å·²çŸ¥é”™è¯¯ä¿®å¤æ–¹æ¡ˆ
            "decisions": [],             # å…³é”®å†³ç­–è®°å½•
            "coverage_strategies": []    # è¦†ç›–ç‡ä¼˜åŒ–ç­–ç•¥
        }
    )

def is_terminal_state(state: FuzzingWorkflowState) -> bool:
    """Check if the workflow has reached a terminal state."""
    
    # Check termination conditions
    if state.get("termination_reason"):
        return True
    
    # Check maximum iterations
    current_iter = state.get("current_iteration", 0)
    max_iter = state.get("max_iterations", 5)
    if current_iter >= max_iter:
        return True
    
    # Check if we have a successful result
    if (state.get("compile_success") and 
        state.get("coverage_results") and 
        state.get("coverage_results", {}).get("coverage", 0) > 0.8):
        return True
    
    return False

def update_token_usage(state: FuzzingWorkflowState, agent_name: str, 
                       prompt_tokens: int, completion_tokens: int, total_tokens: int) -> None:
    """
    Update token usage statistics in state.
    
    Args:
        state: The workflow state
        agent_name: Name of the agent making the call
        prompt_tokens: Number of prompt tokens used
        completion_tokens: Number of completion tokens used
        total_tokens: Total tokens used
    """
    if "token_usage" not in state:
        state["token_usage"] = {
            "total_prompt_tokens": 0,
            "total_completion_tokens": 0,
            "total_tokens": 0,
            "by_agent": {}
        }
    
    # Update totals
    state["token_usage"]["total_prompt_tokens"] += prompt_tokens
    state["token_usage"]["total_completion_tokens"] += completion_tokens
    state["token_usage"]["total_tokens"] += total_tokens
    
    # Update per-agent statistics
    if agent_name not in state["token_usage"]["by_agent"]:
        state["token_usage"]["by_agent"][agent_name] = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0,
            "call_count": 0
        }
    
    agent_stats = state["token_usage"]["by_agent"][agent_name]
    agent_stats["prompt_tokens"] += prompt_tokens
    agent_stats["completion_tokens"] += completion_tokens
    agent_stats["total_tokens"] += total_tokens
    agent_stats["call_count"] += 1


def get_token_usage_summary(state: FuzzingWorkflowState) -> str:
    """Get a formatted summary of token usage."""
    token_usage = state.get("token_usage", {})
    if not token_usage:
        return "No token usage data available"
    
    summary = f"\n{'='*60}\n"
    summary += "Token Usage Summary\n"
    summary += f"{'='*60}\n"
    summary += f"Total Prompt Tokens:     {token_usage.get('total_prompt_tokens', 0):,}\n"
    summary += f"Total Completion Tokens: {token_usage.get('total_completion_tokens', 0):,}\n"
    summary += f"Total Tokens:            {token_usage.get('total_tokens', 0):,}\n"
    
    by_agent = token_usage.get("by_agent", {})
    if by_agent:
        summary += f"\n{'-'*60}\n"
        summary += "By Agent:\n"
        summary += f"{'-'*60}\n"
        for agent_name, stats in sorted(by_agent.items()):
            summary += f"\n{agent_name}:\n"
            summary += f"  Calls:             {stats.get('call_count', 0)}\n"
            summary += f"  Prompt Tokens:     {stats.get('prompt_tokens', 0):,}\n"
            summary += f"  Completion Tokens: {stats.get('completion_tokens', 0):,}\n"
            summary += f"  Total Tokens:      {stats.get('total_tokens', 0):,}\n"
    
    summary += f"{'='*60}\n"
    return summary


def get_state_summary(state: FuzzingWorkflowState) -> str:
    """Get a human-readable summary of the current state."""
    
    benchmark = state.get("benchmark", {})
    project = benchmark.get("project", "unknown")
    function = benchmark.get("function_name", "unknown")
    iteration = state.get("current_iteration", 0)
    status = state.get("workflow_status", "unknown")
    
    summary = f"Fuzzing workflow for {project}::{function} (iteration {iteration}, status: {status})"
    
    # Add build status
    if state.get("compile_success") is not None:
        build_status = "successful" if state["compile_success"] else "failed"
        summary += f"\n  Build: {build_status}"
    
    # Add coverage info
    coverage = state.get("coverage_results", {}).get("coverage")
    if coverage is not None:
        summary += f"\n  Coverage: {coverage:.2%}"
    
    # Add crash info
    crashes = len(state.get("crash_results", []))
    if crashes > 0:
        summary += f"\n  Crashes: {crashes} found"
    
    # Add error info
    errors = len(state.get("errors", []))
    if errors > 0:
        summary += f"\n  Errors: {errors} recorded"
    
    return summary


# ==================== Session Memory Management ====================

def add_api_constraint(
    state: FuzzingWorkflowState,
    constraint: str,
    source: str,
    confidence: str = "medium",
    iteration: int = None
) -> None:
    """
    æ·»åŠ APIçº¦æŸåˆ°session_memoryã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
        constraint: çº¦æŸæè¿°
        source: æ¥æºagentåç§°
        confidence: ç½®ä¿¡åº¦ (high/medium/low)
        iteration: å‘ç°è¯¥çº¦æŸçš„è¿­ä»£è½®æ¬¡
    """
    if "session_memory" not in state:
        state["session_memory"] = {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    api_constraints = state["session_memory"].get("api_constraints", [])
    
    # å»é‡ï¼šæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒçº¦æŸ
    for existing in api_constraints:
        if existing["constraint"] == constraint:
            # å¦‚æœæ–°çº¦æŸçš„ç½®ä¿¡åº¦æ›´é«˜ï¼Œåˆ™æ›´æ–°
            if confidence == "high" and existing["confidence"] != "high":
                existing["confidence"] = "high"
                existing["source"] = source
            return
    
    # æ·»åŠ æ–°çº¦æŸ
    api_constraints.append({
        "constraint": constraint,
        "source": source,
        "confidence": confidence,
        "iteration": iteration if iteration is not None else state.get("current_iteration", 0)
    })
    
    state["session_memory"]["api_constraints"] = api_constraints


def add_known_fix(
    state: FuzzingWorkflowState,
    error_pattern: str,
    solution: str,
    source: str,
    iteration: int = None
) -> None:
    """
    æ·»åŠ å·²çŸ¥é”™è¯¯ä¿®å¤æ–¹æ¡ˆåˆ°session_memoryã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
        error_pattern: é”™è¯¯æ¨¡å¼æè¿°
        solution: è§£å†³æ–¹æ¡ˆ
        source: æ¥æºagentåç§°
        iteration: å‘ç°è¯¥ä¿®å¤çš„è¿­ä»£è½®æ¬¡
    """
    if "session_memory" not in state:
        state["session_memory"] = {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    known_fixes = state["session_memory"].get("known_fixes", [])
    
    # å»é‡
    for existing in known_fixes:
        if existing["error_pattern"] == error_pattern:
            # æ›´æ–°solutionå¦‚æœä¸åŒ
            if existing["solution"] != solution:
                existing["solution"] = solution
                existing["source"] = source
            return
    
    # æ·»åŠ æ–°ä¿®å¤
    known_fixes.append({
        "error_pattern": error_pattern,
        "solution": solution,
        "source": source,
        "iteration": iteration if iteration is not None else state.get("current_iteration", 0)
    })
    
    state["session_memory"]["known_fixes"] = known_fixes


def add_decision(
    state: FuzzingWorkflowState,
    decision: str,
    reason: str,
    source: str,
    iteration: int = None
) -> None:
    """
    æ·»åŠ å…³é”®å†³ç­–è®°å½•åˆ°session_memoryã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
        decision: å†³ç­–å†…å®¹
        reason: å†³ç­–åŸå› 
        source: æ¥æºagentåç§°
        iteration: åšå‡ºå†³ç­–çš„è¿­ä»£è½®æ¬¡
    """
    if "session_memory" not in state:
        state["session_memory"] = {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    decisions = state["session_memory"].get("decisions", [])
    
    decisions.append({
        "decision": decision,
        "reason": reason,
        "source": source,
        "iteration": iteration if iteration is not None else state.get("current_iteration", 0)
    })
    
    # é™åˆ¶decisionsæ•°é‡ï¼Œåªä¿ç•™æœ€è¿‘10æ¡
    state["session_memory"]["decisions"] = decisions[-10:]


def set_archetype(
    state: FuzzingWorkflowState,
    archetype_type: str,
    lifecycle_phases: List[str],
    source: str,
    iteration: int = None
) -> None:
    """
    è®¾ç½®è¯†åˆ«å‡ºçš„APIæ¶æ„æ¨¡å¼ã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
        archetype_type: æ¶æ„ç±»å‹ (ä¾‹å¦‚: "stateful_decoder", "simple_parser")
        lifecycle_phases: ç”Ÿå‘½å‘¨æœŸé˜¶æ®µåˆ—è¡¨
        source: æ¥æºagentåç§°
        iteration: è¯†åˆ«è¯¥æ¶æ„çš„è¿­ä»£è½®æ¬¡
    """
    if "session_memory" not in state:
        state["session_memory"] = {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    state["session_memory"]["archetype"] = {
        "type": archetype_type,
        "lifecycle_phases": lifecycle_phases,
        "source": source,
        "iteration": iteration if iteration is not None else state.get("current_iteration", 0)
    }


def add_coverage_strategy(
    state: FuzzingWorkflowState,
    strategy: str,
    target: str,
    source: str,
    iteration: int = None
) -> None:
    """
    æ·»åŠ è¦†ç›–ç‡ä¼˜åŒ–ç­–ç•¥åˆ°session_memoryã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
        strategy: ç­–ç•¥æè¿°
        target: ç›®æ ‡/é¢„æœŸæ•ˆæœ
        source: æ¥æºagentåç§°
        iteration: æå‡ºè¯¥ç­–ç•¥çš„è¿­ä»£è½®æ¬¡
    """
    if "session_memory" not in state:
        state["session_memory"] = {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    strategies = state["session_memory"].get("coverage_strategies", [])
    
    # å»é‡
    for existing in strategies:
        if existing["strategy"] == strategy:
            return
    
    strategies.append({
        "strategy": strategy,
        "target": target,
        "source": source,
        "iteration": iteration if iteration is not None else state.get("current_iteration", 0)
    })
    
    # é™åˆ¶strategiesæ•°é‡ï¼Œåªä¿ç•™æœ€è¿‘10æ¡
    state["session_memory"]["coverage_strategies"] = strategies[-10:]


def format_session_memory_for_prompt(state: FuzzingWorkflowState) -> str:
    """
    å°†session_memoryæ ¼å¼åŒ–ä¸ºå¯è¯»çš„æ–‡æœ¬ï¼Œç”¨äºæ³¨å…¥åˆ°agentæç¤ºä¸­ã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
    
    Returns:
        æ ¼å¼åŒ–çš„session_memoryæ–‡æœ¬
    """
    session_memory = state.get("session_memory", {})
    
    if not session_memory:
        return "*æœ¬è½®ä»»åŠ¡å°šæ— å…±è¯†çº¦æŸ*"
    
    parts = []
    
    # 1. æ ¼å¼åŒ–APIçº¦æŸ
    if api_constraints := session_memory.get("api_constraints", []):
        parts.append("## APIä½¿ç”¨çº¦æŸ")
        for c in api_constraints:
            confidence_marker = {
                "high": "ğŸ”´",
                "medium": "ğŸŸ¡",
                "low": "ğŸŸ¢"
            }.get(c.get("confidence", "medium"), "")
            parts.append(f"- {confidence_marker} {c['constraint']}")
            parts.append(f"  *æ¥æº: {c['source']}, è½®æ¬¡: {c.get('iteration', 0)}*")
    
    # 2. æ ¼å¼åŒ–æ¶æ„æ¨¡å¼
    if archetype := session_memory.get("archetype"):
        parts.append("\n## å·²è¯†åˆ«æ¶æ„æ¨¡å¼")
        parts.append(f"- **ç±»å‹**: {archetype['type']}")
        parts.append(f"- **ç”Ÿå‘½å‘¨æœŸ**: {' â†’ '.join(archetype['lifecycle_phases'])}")
        parts.append(f"- *æ¥æº: {archetype['source']}, è½®æ¬¡: {archetype.get('iteration', 0)}*")
    
    # 3. æ ¼å¼åŒ–å·²çŸ¥ä¿®å¤
    if known_fixes := session_memory.get("known_fixes", []):
        parts.append("\n## å·²çŸ¥é”™è¯¯ä¿®å¤æ–¹æ¡ˆ")
        for fix in known_fixes[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
            parts.append(f"- **é”™è¯¯**: {fix['error_pattern']}")
            parts.append(f"  **è§£å†³æ–¹æ¡ˆ**: {fix['solution']}")
            parts.append(f"  *æ¥æº: {fix['source']}, è½®æ¬¡: {fix.get('iteration', 0)}*")
    
    # 4. æ ¼å¼åŒ–å†³ç­–è®°å½•
    if decisions := session_memory.get("decisions", []):
        parts.append("\n## å…³é”®å†³ç­–è®°å½•")
        for d in decisions[-3:]:  # åªæ˜¾ç¤ºæœ€è¿‘3æ¡
            parts.append(f"- **å†³ç­–**: {d['decision']}")
            parts.append(f"  **åŸå› **: {d['reason']}")
            parts.append(f"  *æ¥æº: {d['source']}, è½®æ¬¡: {d.get('iteration', 0)}*")
    
    # 5. æ ¼å¼åŒ–è¦†ç›–ç‡ç­–ç•¥
    if strategies := session_memory.get("coverage_strategies", []):
        parts.append("\n## è¦†ç›–ç‡ä¼˜åŒ–ç­–ç•¥")
        for s in strategies[-5:]:  # åªæ˜¾ç¤ºæœ€è¿‘5æ¡
            parts.append(f"- {s['strategy']}")
            parts.append(f"  *ç›®æ ‡: {s['target']}, æ¥æº: {s['source']}*")
    
    if not parts:
        return "*æœ¬è½®ä»»åŠ¡å°šæ— å…±è¯†çº¦æŸ*"
    
    return "\n".join(parts)


def consolidate_session_memory(state: FuzzingWorkflowState) -> Dict[str, Any]:
    """
    æ•´ç†å’Œæ¸…ç†session_memoryï¼Œå»é‡ã€é™åˆ¶é•¿åº¦ç­‰ã€‚
    
    è¿™ä¸ªå‡½æ•°åº”è¯¥åœ¨SupervisorèŠ‚ç‚¹ä¸­è°ƒç”¨ï¼Œç¡®ä¿session_memoryä¿æŒæ•´æ´ã€‚
    
    Args:
        state: å·¥ä½œæµçŠ¶æ€
    
    Returns:
        æ¸…ç†åçš„session_memory
    """
    session_memory = state.get("session_memory", {}).copy()
    
    if not session_memory:
        return {
            "api_constraints": [],
            "archetype": None,
            "known_fixes": [],
            "decisions": [],
            "coverage_strategies": []
        }
    
    # 1. å»é‡APIçº¦æŸ
    if api_constraints := session_memory.get("api_constraints", []):
        # æŒ‰constraintå†…å®¹å»é‡ï¼Œä¿ç•™æœ€é«˜ç½®ä¿¡åº¦çš„
        unique_constraints = {}
        for c in api_constraints:
            key = c["constraint"]
            if key not in unique_constraints:
                unique_constraints[key] = c
            elif c["confidence"] == "high" and unique_constraints[key]["confidence"] != "high":
                unique_constraints[key] = c
        session_memory["api_constraints"] = list(unique_constraints.values())
    
    # 2. å»é‡known_fixes
    if known_fixes := session_memory.get("known_fixes", []):
        unique_fixes = {}
        for fix in known_fixes:
            key = fix["error_pattern"]
            unique_fixes[key] = fix  # åæ¥çš„è¦†ç›–å‰é¢çš„
        session_memory["known_fixes"] = list(unique_fixes.values())[-10:]  # åªä¿ç•™æœ€è¿‘10æ¡
    
    # 3. é™åˆ¶decisionsé•¿åº¦
    if decisions := session_memory.get("decisions", []):
        session_memory["decisions"] = decisions[-10:]
    
    # 4. å»é‡coverage_strategies
    if strategies := session_memory.get("coverage_strategies", []):
        unique_strategies = {}
        for s in strategies:
            key = s["strategy"]
            unique_strategies[key] = s
        session_memory["coverage_strategies"] = list(unique_strategies.values())[-10:]
    
    return session_memory
